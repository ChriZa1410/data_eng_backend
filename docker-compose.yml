---
#version: '3'
services:

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-enterprise-kafka:latest
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker
      - zookeeper
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081

  ingestion_connector:
    image: lsstsqre/cp-kafka-connect:5.5.2-timestampunit
    hostname: ingestion_connector
    container_name: ingestion_connector
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: ingestion_connector
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: ingestion_connector
      #CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-source
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-source
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-source
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      #CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 
      #CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      #CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      #CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      #CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      #CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    volumes:
      - ./simulation_data:/simulation_data

  db_connector:
    image: lsstsqre/cp-kafka-connect:5.5.2-timestampunit
    hostname: db_connector
    container_name: db_connector
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8084:8084"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: db_connector
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: db_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-sink
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-sink
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-sink
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

  metrics_connector:
    image: lsstsqre/cp-kafka-connect:5.5.2-timestampunit
    hostname: metrics_connector
    container_name: metrics_connector
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8085:8085"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: metrics_connector
      CONNECT_REST_PORT: 8085
      CONNECT_GROUP_ID: metrics_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-sink-metrics
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-sink-metrics
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-sink-metrics
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

  control-center:
    image: confluentinc/cp-enterprise-control-center:latest
    hostname: control-center
    container_name: control-center
    depends_on:
      - zookeeper
      - broker
      - schema-registry
      - ingestion_connector
      - db_connector
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      # CONTROL_CENTER_CONNECT_CLUSTER: '["ingestion_connector", "db_connector"]'
      # CONTROL_CENTER_CONNECT_CLUSTER_URL: '["http://ingestion_connector:8083", "http://db_connector:8084"]'
      PORT: 9021

  init-schema-registry:
    build:
      context: ./schema-registry
    depends_on:
      - broker
      - schema-registry

  init-kafka-topics:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - broker
      - schema-registry
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until broker is reachable
      kafka-topics --bootstrap-server broker:29092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_raw_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_upload_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_aggregated_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_processed_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_kafka_metrics --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server broker:29092 --list
      "

  init-databases:
    build:
      context: ./databases
    depends_on:
      - broker
      - influxdb

  IoT_sensor_data_simulation_smartwatch:
    image: confluentinc/cp-kafka:latest
    container_name: IoT_sensor_data_simulation_smartwatch
    depends_on:
      - broker
      - ingestion_connector     
    restart: unless-stopped 
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for ingestion_connector to be launched...'
      cub kafka-ready -b broker:29092 1 20 && sleep 15 && \
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"csv-file-source\",
        \"config\": {
          \"connector.class\": \"FileStreamSource\", 
          \"tasks.max\": \"1\", 
          \"file\": \"/simulation_data/smartwatch_heartrate_source_data.csv\", 
          \"topic\": \"topic_raw_data\",
          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",
          \"value.converter.schemas.enable\": \"false\"
        }
      }' http://ingestion_connector:8083/connectors && \
      sleep infinity
      "
    volumes:
      - ./simulation_data:/simulation_data
    # networks: 
    #   - default


  influxdb:
    image: influxdb:1.8.5
    container_name: influxdb
    volumes:
      - influxdb_data:/var/lib/influxdb
    ports:
      - "8086:8086"
    environment:
      - INFLUXDB_ADMIN_ENABLED=true
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=admin
      - INFLUXDB_USER=admin
      - INFLUXDB_USER_PASSWORD=admin

  kafkaconnect_influxdb_sink_productive:
    image: lsstsqre/kafkaconnect:1.3.1
    container_name: kafkaconnect_influxdb_sink_productive
    depends_on:
      - db_connector
      - influxdb
    entrypoint: kafkaconnect
    environment:
      KAFKA_CONNECT_URL: http://db_connector:8084
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_upload_data

  kafkaconnect_influxdb_sink_metrics:
    image: lsstsqre/kafkaconnect:1.3.1
    container_name: kafkaconnect_influxdb_sink_metrics
    depends_on:
      - metrics_connector
      - influxdb
    entrypoint: kafkaconnect
    environment:
      KAFKA_CONNECT_URL: http://metrics_connector:8085
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_kafka_metrics

  init-kafkaconnect_influxdb_sink_productive:
    image: lsstsqre/kafkaconnect:1.3.1
    container_name: init-kafkaconnect_influxdb_sink_productive
    depends_on:
      - kafkaconnect_influxdb_sink_productive
      - db_connector
    environment:
      KAFKA_CONNECT_URL: http://db_connector:8084
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_upload_data
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for db_connector to be launched...'
      sleep 60

      echo -e 'Creating InfluxDB Sink Instance: Data storage...'
      # create Instance of InfluxDB Sink for productive smartwatch data
      kafkaconnect create influxdb-sink -d data_storage topic_upload_data
      "

  init-kafkaconnect_influxdb_sink_metrics:
    image: lsstsqre/kafkaconnect:1.3.1
    container_name: init-kafkaconnect_influxdb_sink_metrics
    depends_on:
      - kafkaconnect_influxdb_sink_metrics
      - metrics_connector
    environment:
      KAFKA_CONNECT_URL: http://metrics_connector:8085
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_kafka_metrics
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for metrics_connector to be launched...'
      sleep 60

      echo -e 'Creating InfluxDB Sink Instance: Kafka metrics...'
      # create Instance of InfluxDB Sink for Kafka metrics data
      kafkaconnect create influxdb-sink -d kafka_metrics topic_kafka_metrics
      "

  data_simulator:
    build:
      context: ./simulation_data
    depends_on:
      - broker
    ports:
      - 4000:4000
    volumes:
      - ./simulation_data:/simulation_data
    
  data_aggregation:
    build: ./data_aggregation
    depends_on:
      - broker
    #restart: unless-stopped
    environment:
      - KAFKA_BROKER=broker:29092
    networks:
      - default

  data_processing:
    build: ./data_processing
    depends_on:
      - broker
    #restart: unless-stopped
    environment:
      - KAFKA_BROKER=broker:29092
    networks:
      - default

  data_upload:
    build:
      context: ./data_upload
    restart: unless-stopped
    depends_on:
      - broker
      - init-kafka-topics
    ports:
      - 2000:2000

  grafana:
    image: grafana/grafana:latest
    depends_on:
      - influxdb
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  influxdb_data:
  grafana_data:

networks:
  default:
    external:
      name: kafka-network