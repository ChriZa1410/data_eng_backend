---
# Docker-Compose file 
#  
# In this file the single microservices of the cluster are defined as docker containers, configured and all together orchestrated by Docker.
# Configurations of the docker containers depend on used docker image with its version, port allocations, network, volume and environment variables.


services:

  # zookeeper: overall management of kafka cluster
  zookeeper:
    image: confluentinc/cp-zookeeper:${CP_KAFKA_VERSION}
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - ${ZOOKEEPER_HOST_PORT}:${ZOOKEEPER_CONTAINER_PORT}
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CONTAINER_PORT}
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - default

  # broker1: management of data flows/topics in the cluster (three brokers available for replication)
  broker1:
    image: confluentinc/cp-enterprise-kafka:${CP_KAFKA_VERSION}
    hostname: broker1
    container_name: broker1
    depends_on:
      - zookeeper
    restart: unless-stopped
    ports:
      - ${BROKER1_HOST_PORT}:${BROKER1_CONTAINER_PORT}
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:${ZOOKEEPER_CONTAINER_PORT}'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://localhost:${BROKER1_CONTAINER_PORT}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_LISTENERS: INSIDE://0.0.0.0:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://0.0.0.0:${BROKER1_CONTAINER_PORT}
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
    networks:
      - default

  # broker2: management of data flows/topics in the cluster (three brokers available for replication)
  broker2:
    image: confluentinc/cp-enterprise-kafka:${CP_KAFKA_VERSION}
    hostname: broker2
    container_name: broker2
    depends_on:
      - zookeeper
    restart: unless-stopped
    ports:
      - ${BROKER2_HOST_PORT}:${BROKER2_CONTAINER_PORT}
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:${ZOOKEEPER_CONTAINER_PORT}'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://localhost:${BROKER2_CONTAINER_PORT}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_LISTENERS: INSIDE://0.0.0.0:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://0.0.0.0:${BROKER2_CONTAINER_PORT}
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
    networks:
      - default
  
  # broker3: management of data flows/topics in the cluster (three brokers available for replication)
  broker3:
    image: confluentinc/cp-enterprise-kafka:${CP_KAFKA_VERSION}
    hostname: broker3
    container_name: broker3
    depends_on:
      - zookeeper
    restart: unless-stopped
    ports:
      - ${BROKER3_HOST_PORT}:${BROKER3_CONTAINER_PORT}
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:${ZOOKEEPER_CONTAINER_PORT}'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://localhost:${BROKER3_CONTAINER_PORT}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_LISTENERS: INSIDE://0.0.0.0:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},OUTSIDE://0.0.0.0:${BROKER3_CONTAINER_PORT}
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
    networks:
      - default

  # schema-registry: manages data schemas of the topics in the kafka cluster
  schema-registry:
    image: confluentinc/cp-schema-registry:${SCHEMA_REGISTRY_VERSION}
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker1
      - broker2
      - broker3
      - zookeeper
    restart: unless-stopped
    ports:
      - ${SCHEMA_REGISTRY_HOST_PORT}:${SCHEMA_REGISTRY_CONTAINER_PORT}
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
    networks:
      - default

  # ingestion_connector: Source Connect cluster (FileSource) to append external data source (csv file) to the kafka cluster
  ingestion_connector:
    image: lsstsqre/cp-kafka-connect:${LSSTSQRE_CP_CONNECT_VERSION}
    hostname: ingestion_connector
    container_name: ingestion_connector
    depends_on:
      - broker1
      - schema-registry
    ports:
      - ${INFLUXDB_INGESTION_CONNECTOR_HOST_PORT}:${INFLUXDB_INGESTION_CONNECTOR_CONTAINER_PORT}
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
      CONNECT_REST_ADVERTISED_HOST_NAME: ingestion_connector
      CONNECT_REST_PORT: ${INFLUXDB_INGESTION_CONNECTOR_CONTAINER_PORT}
      CONNECT_GROUP_ID: ingestion_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-source
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-source
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-source
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
    volumes:
      - ./data_simulator:/data_simulator
    networks:
      - default

  # db_connector: Sink Connect cluster (InfluxDB Sink) to append external data sink (InfluxDB) to the kafka cluster for storing productive data of the cluster
  db_connector:
    image: lsstsqre/cp-kafka-connect:${LSSTSQRE_CP_CONNECT_VERSION}
    hostname: db_connector
    container_name: db_connector
    depends_on:
      - broker1
      - schema-registry
    ports:
      - ${INFLUXDB_PROD_CONNECTOR_HOST_PORT}:${INFLUXDB_PROD_CONNECTOR_CONTAINER_PORT}
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
      CONNECT_REST_ADVERTISED_HOST_NAME: db_connector
      CONNECT_REST_PORT: ${INFLUXDB_PROD_CONNECTOR_CONTAINER_PORT}
      CONNECT_GROUP_ID: db_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-sink
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-sink
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-sink
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    networks:
      - default  

  # control-center: Frontend hub to manage and check health and functionality of the kafka cluster
  control-center:
    image: confluentinc/cp-enterprise-control-center:${CP_CONTROL_CENTER_VERSION}
    hostname: control-center
    container_name: control-center
    depends_on:
      - zookeeper
      - broker1
      - schema-registry
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    ports:
      - ${CP_CONTROL_CENTER_HOST_PORT}:${CP_CONTROL_CENTER_CONTAINER_PORT}
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:${ZOOKEEPER_CONTAINER_PORT}'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      PORT: ${CP_CONTROL_CENTER_CONTAINER_PORT}
    networks:
      - default

  # init-kafka-topics: defines the topics in the kafka cluster 
  init-kafka-topics:
    image: confluentinc/cp-kafka:${CP_KAFKA_CONNECT_VERSION}
    depends_on:
      - broker1
      - schema-registry
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until broker is reachable
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --create --if-not-exists --topic topic_raw_data --replication-factor 3 --partitions 3
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --create --if-not-exists --topic topic_upload_data --replication-factor 3 --partitions 3
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --create --if-not-exists --topic topic_aggregated_data --replication-factor 3 --partitions 3
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --create --if-not-exists --topic topic_processed_data --replication-factor 3 --partitions 3

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} --list
      "

  # init-schema-registry: defines schemas of the topics in the kafka cluster 
  init-schema-registry:
    build:
      context: ./schema-registry
    depends_on:
      - broker1
      - schema-registry
    restart: unless-stopped

  # init-databases: creates influx databases for storing productive data
  init-databases:
    build:
      context: ./databases
    depends_on:
      - broker1
      - influxdb

  # IoT_sensor_data_simulation_smartwatch: simulates an IoT data source of type smartwatch (is a connector instance inside ingestion_connector cluster)
  IoT_sensor_data_simulation_smartwatch:
    image: confluentinc/cp-kafka:${CP_KAFKA_CONNECT_VERSION}
    container_name: IoT_sensor_data_simulation_smartwatch
    depends_on:
      - broker1
      - ingestion_connector     
    restart: unless-stopped 
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for ingestion_connector to be launched...'
      cub kafka-ready -b broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE} 1 20 && sleep 15 && \
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"csv-file-source\",
        \"config\": {
          \"connector.class\": \"FileStreamSource\", 
          \"tasks.max\": \"1\", 
          \"file\": \"/data_simulator/smartwatch_heartrate_source_data.csv\", 
          \"topic\": \"topic_raw_data\",
          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",
          \"value.converter.schemas.enable\": \"false\"
        }
      }' http://ingestion_connector:${INFLUXDB_INGESTION_CONNECTOR_CONTAINER_PORT}/connectors && \
      sleep infinity
      "
    volumes:
      - ./data_simulator:/data_simulator
    networks:
      - default

  # influxdb: external InfluxDB database for storing productive data 
  influxdb:
    image: influxdb:${INFLUXDB_VERSION}
    container_name: influxdb
    volumes:
      - influxdb_data:/var/lib/influxdb
    ports:
      - ${INFLUXDB_HOST_PORT}:${INFLUXDB_CONTAINER_PORT}
    environment:
      - INFLUXDB_ADMIN_ENABLED=true
      - INFLUXDB_ADMIN_USER=${INFLUXDB_USERNAME}
      - INFLUXDB_ADMIN_PASSWORD=${INFLUXDB_PASSWORD}
      - INFLUXDB_USER=${INFLUXDB_USERNAME}
      - INFLUXDB_USER_PASSWORD=${INFLUXDB_PASSWORD}

  # kafkaconnect_influxdb_sink_productive: stores productive data after being processed in the kafka cluster (is a connector instance inside db_connector cluster)
  kafkaconnect_influxdb_sink_productive:
    image: lsstsqre/kafkaconnect:${LSSTSQRE_CONNECT_VERSION}
    container_name: kafkaconnect_influxdb_sink_productive
    depends_on:
      - db_connector
      - influxdb
    entrypoint: kafkaconnect
    environment:
      KAFKA_CONNECT_URL: http://db_connector:${INFLUXDB_PROD_CONNECTOR_CONTAINER_PORT}
      KAFKA_BROKER_URL: broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:${INFLUXDB_CONTAINER_PORT}
      KAFKA_CONNECT_TOPIC: topic_upload_data

  
  # init-kafkaconnect_influxdb_sink_productive: initializes sink connector instance inside db_connector cluster for productive data
  init-kafkaconnect_influxdb_sink_productive:
    image: lsstsqre/kafkaconnect:${LSSTSQRE_CONNECT_VERSION}
    container_name: init-kafkaconnect_influxdb_sink_productive
    depends_on:
      - kafkaconnect_influxdb_sink_productive
      - db_connector
    restart: unless-stopped 
    environment:
      KAFKA_CONNECT_URL: http://db_connector:${INFLUXDB_PROD_CONNECTOR_CONTAINER_PORT}
      KAFKA_BROKER_URL: broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:${INFLUXDB_CONTAINER_PORT}
      KAFKA_CONNECT_TOPIC: topic_upload_data
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for db_connector to be launched...'
      sleep 10

      echo -e 'Creating InfluxDB Sink Instance: Data storage...'
      # create Instance of InfluxDB Sink for productive smartwatch data
      kafkaconnect create influxdb-sink -d data_storage topic_upload_data
      "


  # data_simulator: provides sample data for smartwatch sensor simulation
  data_simulator:
    build:
      context: ./data_simulator
    depends_on:
      - broker1
    ports:
      - ${DATA_SIMULATOR_HOST_PORT}:${DATA_SIMULATOR_CONTAINER_PORT}
    volumes:
      - ./data_simulator:/data_simulator
    networks:
      - default

  # data_aggregation: fetches raw sensor data and aggregates data to a proper format
  data_aggregation:
    build: ./data_aggregation
    depends_on:
      - broker1
    ports:
      - ${DATA_AGGREGATION_HOST_PORT}:${DATA_AGGREGATION_CONTAINER_PORT}
    restart: unless-stopped
    environment:
      - KAFKA_BROKER='broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
    networks:
      - default

  # data_processing: fetches aggregated sensor data and processes data with some calculations (e.g. max/min/mean values)
  data_processing:
    build: ./data_processing
    depends_on:
      - broker1
    ports:
      - ${DATA_PROCESSING_HOST_PORT}:${DATA_PROCESSING_CONTAINER_PORT}
    restart: unless-stopped
    environment:
      - KAFKA_BROKER='broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
    networks:
      - default

  # data_upload: fetches processed data and uploads data to influxdb data sink
  data_upload:
    build:
      context: ./data_upload
    restart: unless-stopped
    depends_on:
      - broker1
      - init-kafka-topics
    ports:
      - ${DATA_UPLOAD_HOST_PORT}:${DATA_UPLOAD_CONTAINER_PORT}
    environment:
      - KAFKA_BROKER='broker1:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker2:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE},broker3:${BROKER_ADVERTISED_LISTENER_PORT_INSIDE}'
      - SCHEMA_REGISTRY_URL=http://schema-registry:${SCHEMA_REGISTRY_CONTAINER_PORT}
    networks:
      - default

  # grafana: dashboard to monitor IoT sensor data processed inside the kafka cluster and uploaded to database
  grafana:
    image: grafana/grafana:${GRAFANA_VERSION}
    depends_on:
      - influxdb
    restart: unless-stopped
    ports:
      - ${GRAFANA_HOST_PORT}:${GRAFANA_CONTAINER_PORT}
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USERNAME}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./provisioning:/etc/grafana/provisioning
    networks:
      - default

volumes:
  influxdb_data:
  grafana_data:

networks:
  default:
    external:
      name: kafka-network