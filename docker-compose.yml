---
version: '3'
services:

  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.3
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-enterprise-kafka:5.5.3
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker
      - zookeeper
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081

  ingestion_connector:
    image: lsstsqre/cp-kafka-connect:5.5.2-0.9.1
    hostname: ingestion_connector
    container_name: ingestion_connector
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: ingestion_connector
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: ingestion_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-source
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-source
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-source
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    volumes:
      - ./simulation_data:/simulation_data

  db_connector:
    image: lsstsqre/cp-kafka-connect:5.5.2-0.9.1
    hostname: db_connector
    container_name: db_connector
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8084:8084"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: db_connector
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: db_connector
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-sink
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-sink
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status-sink
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/landoop/jars/lib"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

  control-center:
    image: confluentinc/cp-enterprise-control-center:5.4.0
    hostname: control-center
    container_name: control-center
    depends_on:
      - zookeeper
      - broker
      - schema-registry
      - ingestion_connector
      - db_connector
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONTROL_CENTER_CONNECT_CLUSTER: '["ingestion_connector", "db_connector"]'
      CONTROL_CENTER_CONNECT_CLUSTER_URL: '["http://ingestion_connector:8083", "http://db_connector:8084"]'
      PORT: 9021

  init-schema-registry:
    build:
      context: ./schema-registry
    depends_on:
      - broker
      - schema-registry

  init-kafka-topics:
    image: confluentinc/cp-kafka:6.1.1
    depends_on:
      - broker
      - schema-registry
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until broker is reachable
      kafka-topics --bootstrap-server broker:29092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_raw_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic topic_upload_data --replication-factor 1 --partitions 1
    
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server broker:29092 --list
      "

  IoT_sensor_data_simulation_smartwatch:
    image: confluentinc/cp-kafka:latest
    container_name: IoT_sensor_data_simulation_smartwatch
    depends_on:
      - broker
      - ingestion_connector     
    restart: unless-stopped 
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for ingestion_connector to be launched...'
      cub kafka-ready -b broker:29092 1 20 && sleep 15 && \
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"csv-file-source\",
        \"config\": {
          \"connector.class\": \"FileStreamSource\", 
          \"tasks.max\": \"1\", 
          \"file\": \"/simulation_data/smartwatch_heartrate_source.csv\", 
          \"topic\": \"topic_raw_data\", 
          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter.schema.registry.url\": \"http://schema-registry:8081\"
        }
      }' http://ingestion_connector:8083/connectors && \
      sleep infinity
      "
    volumes:
      - ./simulation_data:/simulation_data

  influxdb:
    image: influxdb:1.8.5
    container_name: influxdb
    volumes:
      - influxdb_data:/var/lib/influxdb
    ports:
      - "8086:8086"
    environment:
      - INFLUXDB_DB=data_storage
      - INFLUXDB_ADMIN_ENABLED=true
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=admin
      - INFLUXDB_USER=admin
      - INFLUXDB_USER_PASSWORD=admin

  kafkaconnect_influxdb_sink:
    image: lsstsqre/kafkaconnect:0.9.1
    container_name: kafkaconnect_influxdb_sink
    depends_on:
      - db_connector
      - influxdb
    entrypoint: kafkaconnect
    environment:
      KAFKA_CONNECT_URL: http://db_connector:8084
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_upload_data

  init-kafkaconnect_influxdb_sink:
    image: lsstsqre/kafkaconnect:0.9.1
    container_name: init-kafkaconnect_influxdb_sink
    depends_on:
      - kafkaconnect_influxdb_sink
      - db_connector
    environment:
      KAFKA_CONNECT_URL: http://db_connector:8084
      KAFKA_BROKER_URL: broker:29092
      KAFKA_CONNECT_INFLUXDB_URL: http://influxdb:8086
      KAFKA_CONNECT_TOPIC: topic_upload_data
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Waiting for db_connector to be launched...'

      sleep 60
      echo -e 'Creating InfluxDB Sink Instance: Data storage...'

      # create Instance of InfluxDB Sink
      kafkaconnect create influxdb-sink -d data_storage topic_upload_data
      "
  
  data_upload:
    build:
      context: ./data_upload
    restart: unless-stopped
    depends_on:
      - broker
      - init-kafka-topics
    ports:
      - 2000:2000
  

  # data_consumer:
  #   build:
  #     context: ./data_consumer
  #   restart: unless-stopped
  #   depends_on:
  #     - broker
  #   ports:
  #     - 3000:3000
    # networks: 
    #   - kafka_network


volumes:
  influxdb_data: